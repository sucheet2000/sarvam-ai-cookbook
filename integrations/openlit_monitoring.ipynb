{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Sarvam AI usage with OpenLIT: OpenTelemetry-Native Observability\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "This cookbook demonstrates how to implement comprehensive observability for Sarvam AI applications using **OpenLIT**, an OpenTelemetry-native LLM monitoring platform. You'll learn how to:\n",
    "\n",
    "- Auto-instrument Sarvam AI API calls with zero code changes\n",
    "- Track costs, performance, and token usage in real-time\n",
    "- Capture detailed traces of chat completions, translations, and speech services\n",
    "- Visualize and analyze your AI application's behavior\n",
    "- Debug issues with complete request/response visibility\n",
    "\n",
    "By the end of this tutorial, you'll have a production-ready monitoring setup that provides deep insights into your Sarvam AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Why Monitor Your AI Applications?**\n",
    "\n",
    "### **The Challenge of AI Observability**\n",
    "\n",
    "AI-powered applications introduce unique observability challenges that traditional monitoring tools weren't designed to handle. When your Sarvam AI application fails or behaves unexpectedly, you need answers:\n",
    "\n",
    "- **Which API call failed and why?** - Understanding failure modes across chat completions, translations, and speech services\n",
    "- **What was the model thinking?** - Visibility into prompts, responses, and reasoning patterns\n",
    "- **How much is each request costing?** - Real-time cost tracking across different Sarvam models and services\n",
    "- **Which calls succeeded and which failed?** - Success rates, error patterns, and reliability metrics\n",
    "- **How long did each operation take?** - Performance bottlenecks in LLM inference, translation, or speech processing\n",
    "- **What inputs led to this behavior?** - Complete context of user messages, system prompts, and model parameters\n",
    "\n",
    "Without proper observability, debugging means adding print statements, guessing at failure points, and blindly optimizing costs. You're essentially flying blind through complex AI workflows.\n",
    "\n",
    "### **The Hidden Complexity of AI Failures**\n",
    "\n",
    "Consider a typical scenario: Your multilingual chatbot using Sarvam AI fails to respond correctly in Hindi. Without proper tracing, you're left wondering:\n",
    "\n",
    "- Did the translation service fail?\n",
    "- Was the language detection incorrect?\n",
    "- Did the chat model misunderstand the context?\n",
    "- Was there a quota or rate limit issue?\n",
    "- Did the request timeout or fail silently?\n",
    "\n",
    "Each of these failures requires different solutions, but without visibility into execution, you're reduced to trial-and-error debugging.\n",
    "\n",
    "### **The Role of OpenTelemetry in AI Observability**\n",
    "\n",
    "OpenTelemetry has emerged as the industry standard for observability, offering a vendor-neutral approach to collecting traces, metrics, and logs. For AI applications, OpenTelemetry's structured tracing is particularly powerful because it can capture:\n",
    "\n",
    "- **Distributed execution flows** - Following requests across multiple services and components\n",
    "- **Hierarchical relationships** - Parent-child relationships between operations (API call → model inference → response)\n",
    "- **Rich contextual data** - Arbitrary attributes attached to each span for detailed analysis\n",
    "- **Performance metrics** - Timing data at every level of the execution stack\n",
    "- **Standard semantic conventions** - Consistent attribute naming for AI workloads (model name, token counts, prompts, completions)\n",
    "\n",
    "### **Why OpenTelemetry Matters for AI Applications**\n",
    "\n",
    "Traditional APM tools excel at monitoring web servers and databases, but they fall short with AI applications because:\n",
    "\n",
    "- **AI workflows are non-deterministic** - The same input can produce different outputs, making traditional error tracking insufficient\n",
    "- **Context is everything** - You need to see not just that a call failed, but what the model was processing and how it responded\n",
    "- **Token costs are variable** - Unlike fixed compute costs, AI costs vary dramatically based on prompt size and model choice\n",
    "- **Multi-step reasoning** - Complex applications chain multiple AI services (translation → chat → speech), requiring full execution history\n",
    "\n",
    "### **What OpenLIT Provides**\n",
    "\n",
    "OpenLIT builds on OpenTelemetry to provide AI-specific observability:\n",
    "\n",
    "- **Zero-code auto-instrumentation** - Monitor Sarvam AI calls without modifying your application code\n",
    "- **Cost tracking** - Real-time visibility into token usage and estimated costs\n",
    "- **Performance monitoring** - Latency tracking, throughput analysis, and bottleneck identification\n",
    "- **Request/response capture** - Complete visibility into prompts, completions, and model parameters\n",
    "- **Error tracking** - Detailed error logs with full context for debugging\n",
    "- **Multi-backend support** - Send telemetry to Grafana, Datadog, New Relic, or any OpenTelemetry-compatible backend\n",
    "\n",
    "Let's dive into implementing this powerful monitoring for your Sarvam AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install the required packages:\n",
    "- `sarvamai` - Official Sarvam AI Python SDK\n",
    "- `openlit` - OpenTelemetry-native LLM observability SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq sarvamai openlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup with OpenLIT\n",
    "\n",
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlit\n",
    "from sarvamai import SarvamAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Your API Key\n",
    "\n",
    "Get your Sarvam AI API key from the [Sarvam AI Dashboard](https://dashboard.sarvam.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARVAM_API_KEY = \"YOUR_SARVAM_API_KEY\"\n",
    "os.environ[\"SARVAM_API_KEY\"] = SARVAM_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenLIT Monitoring\n",
    "\n",
    "This single line of code enables automatic instrumentation of all Sarvam AI API calls. OpenLIT will capture:\n",
    "- Request parameters (model, messages, temperature, etc.)\n",
    "- Response data (completions, token counts, etc.)\n",
    "- Timing information (latency, duration)\n",
    "- Cost estimates (based on token usage)\n",
    "- Error details (if any failures occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenLIT with console output for development\n",
    "openlit.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Sarvam AI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = SarvamAI(api_subscription_key=SARVAM_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Chat Completions\n",
    "\n",
    "### Basic Chat Completion with Tracing\n",
    "\n",
    "Let's make a simple chat completion request. OpenLIT will automatically capture all the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a chat completion request\n",
    "response = client.chat.completions(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant knowledgeable about Indian culture.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the major classical dance forms of India?\"}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(\"\\n--- OpenLIT is capturing this interaction in the background ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-turn Conversation with Context Tracking\n",
    "\n",
    "OpenLIT tracks the entire conversation context across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Bharatanatyam.\"}\n",
    "]\n",
    "\n",
    "# First turn\n",
    "response1 = client.chat.completions(messages=conversation, temperature=0.7)\n",
    "print(\"Turn 1:\", response1.choices[0].message.content[:200] + \"...\")\n",
    "\n",
    "# Add assistant's response to conversation\n",
    "conversation.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": response1.choices[0].message.content\n",
    "})\n",
    "\n",
    "# Second turn\n",
    "conversation.append({\"role\": \"user\", \"content\": \"What are the key mudras used?\"})\n",
    "response2 = client.chat.completions(messages=conversation, temperature=0.7)\n",
    "print(\"\\nTurn 2:\", response2.choices[0].message.content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Different Reasoning Levels\n",
    "\n",
    "Track performance differences between reasoning modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High reasoning effort request\n",
    "complex_question = \"Explain the philosophical differences between Advaita and Dvaita Vedanta.\"\n",
    "\n",
    "response = client.chat.completions(\n",
    "    messages=[{\"role\": \"user\", \"content\": complex_question}],\n",
    "    temperature=0.5,\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content[:300] + \"...\")\n",
    "print(\"\\n--- Check OpenLIT dashboard to compare latency and token usage across reasoning levels ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia-Grounded Queries\n",
    "\n",
    "Monitor requests that use Wikipedia grounding for factual accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia-grounded query\n",
    "response = client.chat.completions(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the history of the Taj Mahal?\"}],\n",
    "    temperature=0.2,\n",
    "    wiki_grounding=True\n",
    ")\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Configuration: Sending Traces to OpenLIT Platform\n",
    "\n",
    "### Deploy OpenLIT Platform Locally\n",
    "\n",
    "For production use, deploy the OpenLIT platform to visualize and analyze your traces. Run this in your terminal:\n",
    "\n",
    "```bash\n",
    "# Clone OpenLIT repository\n",
    "git clone https://github.com/openlit/openlit.git\n",
    "cd openlit\n",
    "\n",
    "# Start OpenLIT with Docker Compose\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "Access the OpenLIT dashboard at `http://127.0.0.1:3000`\n",
    "\n",
    "Default credentials:\n",
    "- Email: `user@openlit.io`\n",
    "- Password: `openlituser`\n",
    "\n",
    "### Configure OpenLIT to Send Traces to Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize OpenLIT to send traces to platform\n",
    "openlit.init(\n",
    "    otlp_endpoint=\"http://127.0.0.1:4318\",  # OpenLIT platform endpoint\n",
    "    application_name=\"sarvam-ai-app\",        # Your application name\n",
    "    environment=\"development\",               # Environment (development/staging/production)\n",
    ")\n",
    "\n",
    "print(\"OpenLIT configured to send traces to platform at http://127.0.0.1:3000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Platform Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some requests that will be visible in the OpenLIT dashboard\n",
    "test_queries = [\n",
    "    \"What are the benefits of yoga?\",\n",
    "    \"Explain the Indian monsoon season.\",\n",
    "    \"What are the main ingredients in biryani?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    response = client.chat.completions(\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response.choices[0].message.content[:100]}...\\n\")\n",
    "\n",
    "print(\"\\n✅ Check your OpenLIT dashboard at http://127.0.0.1:3000 to see these traces!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Other Observability Backends\n",
    "\n",
    "OpenLIT can send traces to any OpenTelemetry-compatible backend.\n",
    "\n",
    "### Grafana Cloud Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Configure for Grafana Cloud\n",
    "# openlit.init(\n",
    "#     otlp_endpoint=\"https://otlp-gateway-prod-us-central-0.grafana.net/otlp\",\n",
    "#     otlp_headers=\"base64encodedkey\"\n",
    "#     application_name=\"sarvam-ai-production\",\n",
    "#     environment=\"production\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datadog Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Configure for Datadog\n",
    "# openlit.init(\n",
    "#     otlp_endpoint=\"https://api.datadoghq.com\",\n",
    "#     otlp_headers=\"YOUR_DATADOG_API_KEY\"\n",
    "#     application_name=\"sarvam-ai-production\",\n",
    "#     environment=\"production\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Relic Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Configure for New Relic\n",
    "# openlit.init(\n",
    "#     otlp_endpoint=\"https://otlp.nr-data.net:4318\",\n",
    "#     otlp_headers=\"YOUR_NEW_RELIC_LICENSE_KEY\"\n",
    "#     application_name=\"sarvam-ai-production\",\n",
    "#     environment=\"production\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What OpenLIT Captures\n",
    "\n",
    "For each Sarvam AI API call, OpenLIT automatically captures:\n",
    "\n",
    "### **Request Attributes**\n",
    "- Model name (e.g., `sarvam-m`)\n",
    "- Temperature, top_p, and other parameters\n",
    "- Reasoning effort level\n",
    "- Wikipedia grounding status\n",
    "- Complete message history\n",
    "- System prompts and user messages\n",
    "\n",
    "### **Response Attributes**\n",
    "- Complete model response/completion\n",
    "- Token counts (prompt tokens, completion tokens, total)\n",
    "- Response time and latency\n",
    "- Finish reason (completed, length, error, etc.)\n",
    "\n",
    "### **Performance Metrics**\n",
    "- Request duration (total time)\n",
    "- Time to first token (for streaming)\n",
    "- Tokens per second\n",
    "- API call success/failure rates\n",
    "\n",
    "### **Cost Information**\n",
    "- Estimated cost per request\n",
    "- Token usage breakdown\n",
    "- Cumulative costs over time\n",
    "\n",
    "### **Error Details**\n",
    "- Exception type and message\n",
    "- Stack traces\n",
    "- HTTP status codes\n",
    "- Error context (request that caused the error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the OpenLIT Dashboard\n",
    "\n",
    "Once you've deployed the OpenLIT platform and sent traces to it, you can:\n",
    "\n",
    "### View Request Traces\n",
    "- Navigate to the **Traces** tab\n",
    "- Filter by application name, environment, or time range\n",
    "- Click on individual traces to see detailed span information\n",
    "- View complete request/response payloads\n",
    "\n",
    "### Analyze Performance\n",
    "- Check the **Metrics** dashboard for:\n",
    "  - Average response time trends\n",
    "  - Token usage patterns\n",
    "  - Request volume over time\n",
    "  - Error rates and types\n",
    "\n",
    "### Track Costs\n",
    "- View the **Cost Analysis** dashboard\n",
    "- See cost breakdown by:\n",
    "  - Model type\n",
    "  - Time period\n",
    "  - User or session\n",
    "  - Feature or endpoint\n",
    "\n",
    "### Debug Errors\n",
    "- Use the **Exceptions** tab to:\n",
    "  - Identify common error patterns\n",
    "  - See full error context\n",
    "  - Track error resolution over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Use Cases\n",
    "\n",
    "### **Use Case 1: Cost Optimization**\n",
    "\n",
    "Use OpenLIT to identify expensive requests and optimize your usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare costs between different approaches\n",
    "\n",
    "# Approach 1: High reasoning effort\n",
    "response1 = client.chat.completions(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize the Indian Constitution.\"}],\n",
    "    reasoning_effort=\"high\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# Approach 2: Medium reasoning effort\n",
    "response2 = client.chat.completions(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize the Indian Constitution.\"}],\n",
    "    reasoning_effort=\"medium\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# Approach 3: Low reasoning effort\n",
    "response3 = client.chat.completions(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize the Indian Constitution.\"}],\n",
    "    reasoning_effort=\"low\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(\"Check OpenLIT dashboard to compare token usage and costs across reasoning levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use Case 2: Performance Benchmarking**\n",
    "\n",
    "Track latency across different types of requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark different query types\n",
    "test_cases = [\n",
    "    (\"Simple query\", \"What is 2+2?\"),\n",
    "    (\"Medium query\", \"Explain the water cycle in 3 sentences.\"),\n",
    "    (\"Complex query\", \"Analyze the economic impact of digital payments in India.\"),\n",
    "]\n",
    "\n",
    "for test_name, query in test_cases:\n",
    "    start = time.time()\n",
    "    response = client.chat.completions(\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    duration = time.time() - start\n",
    "    print(f\"{test_name}: {duration:.2f}s\")\n",
    "\n",
    "print(\"\\nView detailed timing breakdowns in OpenLIT dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use Case 3: A/B Testing and Experimentation**\n",
    "\n",
    "Use OpenLIT to compare different prompt strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B test: Different system prompts\n",
    "\n",
    "# Version A: Concise style\n",
    "response_a = client.chat.completions(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Answer concisely in 1-2 sentences.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is artificial intelligence?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Version B: Detailed style\n",
    "response_b = client.chat.completions(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Provide detailed, comprehensive explanations.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is artificial intelligence?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Version A (concise):\", response_a.choices[0].message.content)\n",
    "print(\"\\nVersion B (detailed):\", response_b.choices[0].message.content)\n",
    "print(\"\\nCompare token usage and costs in OpenLIT to determine the optimal approach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with privacy mode (disables prompt/response capture)\n",
    "openlit.init(\n",
    "    otlp_endpoint=\"http://127.0.0.1:4318\",\n",
    "    application_name=\"sarvam-ai-app\",\n",
    "    environment=\"production\",\n",
    "    disable_metrics_collection=False,\n",
    "    trace_content=False,  # Disables capturing prompts and responses\n",
    ")\n",
    "\n",
    "print(\"OpenLIT configured with privacy mode - metrics tracked but content not logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### **OpenLIT Documentation**\n",
    "- Official Docs: [docs.openlit.io](https://docs.openlit.io)\n",
    "- GitHub: [github.com/openlit/openlit](https://github.com/openlit/openlit)\n",
    "- Discord Community: [Join Discord](https://discord.gg/openlit)\n",
    "\n",
    "### **Sarvam AI Documentation**\n",
    "- API Reference: [docs.sarvam.ai](https://docs.sarvam.ai)\n",
    "- Dashboard: [dashboard.sarvam.ai](https://dashboard.sarvam.ai)\n",
    "- Discord Community: [Join Discord](https://discord.gg/hTuVuPNF)\n",
    "\n",
    "### **OpenTelemetry Resources**\n",
    "- OpenTelemetry Docs: [opentelemetry.io](https://opentelemetry.io)\n",
    "- LLM Observability Guide: [opentelemetry.io/blog/2024/llm-observability](https://opentelemetry.io/blog/2024/llm-observability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have monitoring set up:\n",
    "\n",
    "1. **Explore Your Data** - Make various API calls and explore the traces in OpenLIT dashboard\n",
    "2. **Set Up Alerts** - Configure alerts for critical metrics in your observability backend\n",
    "3. **Optimize Performance** - Use insights to optimize prompt design, model selection, and parameters\n",
    "4. **Track Costs** - Monitor spending and set up budget alerts\n",
    "5. **Share with Team** - Set up shared dashboards for your engineering team\n",
    "\n",
    "### **Production Deployment Checklist**\n",
    "\n",
    "- [ ] Configure production OTLP endpoint\n",
    "- [ ] Set up authentication/API keys\n",
    "- [ ] Enable batching for better performance\n",
    "- [ ] Configure sampling for high-volume applications\n",
    "- [ ] Set up alerts for critical metrics\n",
    "- [ ] Document monitoring setup for your team\n",
    "- [ ] Test error scenarios and verify error tracking\n",
    "- [ ] Configure privacy settings if handling sensitive data\n",
    "- [ ] Set up cost monitoring dashboards\n",
    "- [ ] Schedule regular performance reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully set up comprehensive monitoring for your Sarvam AI applications using OpenLIT! With this setup, you now have:\n",
    "\n",
    "- **Complete visibility** into your AI application's behavior\n",
    "- **Real-time cost tracking** to manage your AI budget\n",
    "- **Performance insights** to optimize latency and throughput\n",
    "- **Error tracking** with full context for faster debugging\n",
    "- **Industry-standard observability** using OpenTelemetry\n",
    "\n",
    "Remember: Observability is not just about monitoring - it's about understanding your system's behavior, optimizing performance, and building more reliable AI applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
