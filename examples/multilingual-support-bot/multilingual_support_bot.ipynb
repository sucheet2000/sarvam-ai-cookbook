{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# **Multilingual Customer Support Bot**\n\nThis notebook demonstrates how to build a full voice loop for Indian-language customer\nsupport using Sarvam AI's speech and language models.\n\n### **Use Case**\nAutomate multilingual customer support for telecom, fintech, and e-commerce companies\nserving India's diverse linguistic population.\n1. **Transcribe:** Use **Sarvam STT (Saarika v2.5)** to convert a customer voice query to text.\n2. **Respond:** Use **Sarvam-M** to generate a helpful support reply in the customer's language.\n3. **Speak:** Use **Bulbul v3 TTS** to synthesize the reply as a WAV audio file.\n\n### **Supported Languages**\n\n| Language | Code | STT | TTS |\n| :--- | :--- | :--- | :--- |\n| Hindi | hi-IN | Yes | Yes |\n| Tamil | ta-IN | Yes | Yes |\n| Telugu | te-IN | Yes | Yes |\n| Kannada | kn-IN | Yes | Yes |\n| Malayalam | ml-IN | Yes | Yes |\n| Gujarati | gu-IN | Yes | Yes |\n| Marathi | mr-IN | Yes | Yes |\n| Bengali | bn-IN | Yes | Yes |\n| English (India) | en-IN | Yes | Yes |"
  },
  {
   "cell_type": "code",
   "id": "cell-1",
   "metadata": {},
   "source": "# Pinning versions for reproducibility\n!pip install -Uqq sarvamai>=0.1.24 python-dotenv>=1.0.0 scipy>=1.10.0 numpy>=1.24.0",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "### **1. Setup & API Key**\n\nObtain your API key from the [Sarvam AI Dashboard](https://dashboard.sarvam.ai).\nCreate a `.env` file in this directory with `SARVAM_API_KEY=your_key_here`, or set the\nenvironment variable directly."
  },
  {
   "cell_type": "code",
   "id": "cell-3",
   "metadata": {},
   "source": "from __future__ import annotations\n\nimport base64\nimport mimetypes\nimport os\nimport traceback\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\nfrom sarvamai import SarvamAI\n\nload_dotenv()\n\nSARVAM_API_KEY = os.environ.get(\"SARVAM_API_KEY\", \"\")\nif not SARVAM_API_KEY or SARVAM_API_KEY == \"YOUR_SARVAM_API_KEY\":\n    raise RuntimeError(\n        \"SARVAM_API_KEY is not set. Add it to your .env file or set the environment variable.\"\n    )\n\nclient = SarvamAI(api_subscription_key=SARVAM_API_KEY)\n\nprint(\"Client initialised.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-4-md",
   "metadata": {},
   "source": "### **2. Step 1 — TRANSCRIBE: Speech-to-Text**\n\n`transcribe_query` sends the audio file to **Sarvam STT (Saarika v2.5)** and returns the\ntranscribed text along with the detected BCP-47 language code.\n\nSupported input: WAV (16 kHz mono recommended) or MP3."
  },
  {
   "cell_type": "code",
   "id": "cell-4",
   "metadata": {},
   "source": "_LANGUAGE_LABELS = {\n    \"hi-IN\": \"Hindi\",\n    \"ta-IN\": \"Tamil\",\n    \"te-IN\": \"Telugu\",\n    \"kn-IN\": \"Kannada\",\n    \"ml-IN\": \"Malayalam\",\n    \"gu-IN\": \"Gujarati\",\n    \"mr-IN\": \"Marathi\",\n    \"bn-IN\": \"Bengali\",\n    \"en-IN\": \"English (India)\",\n}\n\n\ndef transcribe_query(file_path: str) -> tuple[str, str]:\n    \"\"\"Transcribe a customer voice query using Sarvam STT (Saarika v2.5).\n\n    Args:\n        file_path: Path to a WAV or MP3 audio file.\n\n    Returns:\n        Tuple of (transcript, language_code) where language_code is a BCP-47 code\n        such as 'hi-IN', 'ta-IN', or 'en-IN'. Defaults to 'hi-IN' if not detected.\n    \"\"\"\n    path = Path(file_path)\n    content_type = mimetypes.guess_type(str(path))[0] or 'audio/wav'\n    with open(path, 'rb') as audio_file:\n        response = client.speech_to_text.transcribe(\n            file=(path.name, audio_file, content_type),\n            model=\"saarika:v2.5\",\n        )\n\n    transcript    = (getattr(response, 'transcript', '') or '').strip()\n    language_code = getattr(response, 'language_code', 'hi-IN') or 'hi-IN'\n\n    if not transcript:\n        print(\"WARNING: STT returned an empty transcript. Audio quality may be insufficient.\")\n\n    label = _LANGUAGE_LABELS.get(language_code, language_code)\n    print(f\"Transcript ({label}): {transcript!r}\")\n    return transcript, language_code\n\n\nprint(\"transcribe_query defined.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-5-md",
   "metadata": {},
   "source": "### **3. Step 2 — RESPOND: Support Agent**\n\n`generate_response` sends the transcribed query to **Sarvam-M** with a system prompt\nthat instructs the model to reply as a helpful Indian customer support agent **in the\nsame language** as the customer.\n\nIf the transcript is empty (e.g. from a low-quality audio), the agent defaults to a\nHindi greeting so the pipeline still produces a valid audio reply."
  },
  {
   "cell_type": "code",
   "id": "cell-5",
   "metadata": {},
   "source": "SUPPORT_SYSTEM_PROMPT = (\n    \"You are a helpful and polite Indian customer support agent for a telecom company. \"\n    \"Your role is to assist customers with queries about mobile plans, billing, data packs, \"\n    \"recharges, network issues, and general account questions.\\n\\n\"\n    \"Instructions:\\n\"\n    \"- Always respond in the SAME language as the customer's query.\\n\"\n    \"- If the customer queries in Hindi, respond in Hindi. If in Tamil, respond in Tamil. \"\n    \"Apply this rule for all Indian languages.\\n\"\n    \"- Keep responses concise (2-3 sentences) and friendly.\\n\"\n    \"- If the query is unclear or empty, greet the customer warmly in Hindi and ask how \"\n    \"you can help.\\n\"\n    \"- Do not switch to English unless the customer queries in English.\"\n)\n\n_FALLBACK_QUERY = \"\\u0928\\u092e\\u0938\\u094d\\u0924\\u0947, \\u092e\\u0941\\u091d\\u0947 \\u0938\\u0939\\u093e\\u092f\\u0924\\u093e \\u091a\\u093e\\u0939\\u093f\\u090f\\u0964\"\n\n\ndef generate_response(transcript: str, language_code: str) -> str:\n    \"\"\"Generate a customer support response using Sarvam-M.\n\n    The model replies in the same language as the customer query.\n    An empty transcript falls back to a Hindi greeting so the pipeline\n    always produces a valid response.\n    \"\"\"\n    user_message = transcript.strip() if transcript.strip() else _FALLBACK_QUERY\n\n    response = client.chat.completions(\n        messages=[\n            {\"role\": \"system\", \"content\": SUPPORT_SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": user_message},\n        ]\n    )\n\n    if not response or not response.choices:\n        raise ValueError(\"Sarvam-M returned no response. Check your API quota.\")\n\n    content = response.choices[0].message.content\n    if content is None:\n        raise ValueError(\"Sarvam-M returned an empty message content.\")\n\n    reply = content.strip()\n    label = _LANGUAGE_LABELS.get(language_code, language_code)\n    print(f\"Response ({label}): {reply}\")\n    return reply\n\n\nprint(\"generate_response defined.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-6-md",
   "metadata": {},
   "source": "### **4. Step 3 — SPEAK: Text-to-Speech**\n\n`speak_response` converts the support reply to audio using **Bulbul v3** and saves the\nWAV file to the `outputs/` folder.\n\nEach language is paired with a natural-sounding speaker voice. The function falls back\nto `shubh` (Hindi) for any unrecognised language code."
  },
  {
   "cell_type": "code",
   "id": "cell-6",
   "metadata": {},
   "source": "_SPEAKER_MAP = {\n    \"hi-IN\": \"shubh\",\n    \"ta-IN\": \"kavya\",\n    \"te-IN\": \"priya\",\n    \"kn-IN\": \"arvind\",\n    \"ml-IN\": \"anu\",\n    \"gu-IN\": \"priya\",\n    \"mr-IN\": \"shubh\",\n    \"bn-IN\": \"priya\",\n    \"en-IN\": \"shubh\",\n}\n\n\ndef speak_response(\n    text: str,\n    language_code: str,\n    output_dir: str = \"outputs\",\n) -> str:\n    \"\"\"Convert a support response to audio using Bulbul v3 TTS.\n\n    Args:\n        text:          The response text to synthesize.\n        language_code: BCP-47 language code (e.g. 'hi-IN').\n        output_dir:    Directory where the WAV file is saved.\n\n    Returns:\n        Path to the saved WAV file.\n    \"\"\"\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n    speaker = _SPEAKER_MAP.get(language_code, 'shubh')\n\n    tts_response = client.text_to_speech.convert(\n        text=text,\n        target_language_code=language_code,\n        model=\"bulbul:v3\",\n        speaker=speaker,\n        speech_sample_rate=24000,\n    )\n\n    if not tts_response.audios:\n        raise RuntimeError(\n            f\"Bulbul TTS returned no audio for language {language_code}. \"\n            \"Check that the language code and speaker are supported.\"\n        )\n\n    audio_bytes = base64.b64decode(tts_response.audios[0])\n    output_path = str(Path(output_dir) / f\"response_{language_code}.wav\")\n    with open(output_path, 'wb') as f:\n        f.write(audio_bytes)\n\n    print(f\"Audio response saved to: {output_path}\")\n    return output_path\n\n\nprint(\"speak_response defined.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-7-md",
   "metadata": {},
   "source": "### **5. End-to-End Pipeline**\n\n`handle_customer_query` ties all three steps together.\nPass any WAV or MP3 file path and receive a dict with the transcript,\ndetected language, response text, and path to the synthesized audio reply."
  },
  {
   "cell_type": "code",
   "id": "cell-7",
   "metadata": {},
   "source": "def handle_customer_query(\n    audio_path: str,\n    output_dir: str = \"outputs\",\n) -> dict | None:\n    \"\"\"Full voice pipeline: transcribe -> respond -> speak.\n\n    Args:\n        audio_path: Path to a WAV or MP3 audio file of the customer query.\n        output_dir: Directory where the TTS audio reply is saved.\n\n    Returns:\n        Dict with keys 'transcript', 'language_code', 'response_text', 'audio_path',\n        or None if the pipeline fails.\n    \"\"\"\n    print(f\"Processing query: {audio_path}\")\n    try:\n        print(\"  Step 1/3 — Transcribing customer query with Saarika STT...\")\n        transcript, language_code = transcribe_query(audio_path)\n\n        print(\"  Step 2/3 — Generating support response with Sarvam-M...\")\n        response_text = generate_response(transcript, language_code)\n\n        print(\"  Step 3/3 — Synthesizing audio reply with Bulbul TTS...\")\n        audio_out = speak_response(response_text, language_code, output_dir)\n\n        return {\n            \"transcript\":    transcript,\n            \"language_code\": language_code,\n            \"response_text\": response_text,\n            \"audio_path\":    audio_out,\n        }\n\n    except Exception as e:\n        traceback.print_exc()\n        print(f\"ERROR: Failed to process query: {e}\")\n        return None\n\n\nprint(\"handle_customer_query defined.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-8-md",
   "metadata": {},
   "source": "### **6. Demo — Run the Pipeline**\n\nThe cell below generates a synthetic WAV file using `scipy` — no microphone or real\nrecording required — then runs the full pipeline on it.\n\n> **Note:** A programmatically generated audio signal will yield a minimal STT transcript.\n> The pipeline is designed to handle this gracefully: an empty transcript triggers a\n> Hindi greeting so that Sarvam-M and Bulbul TTS always produce a valid audio reply.\n> In production, replace the synthetic WAV with a real customer recording."
  },
  {
   "cell_type": "code",
   "id": "cell-8",
   "metadata": {},
   "source": "import numpy as np\nfrom scipy.io import wavfile\n\n\ndef _create_sample_query(\n    output_path: str = \"sample_data/sample_query_hi.wav\",\n    sample_rate: int = 16000,\n    duration: float = 2.5,\n) -> str:\n    \"\"\"Write a synthetic speech-like WAV file for demo purposes.\n\n    The signal mixes several frequencies to loosely approximate the spectral\n    envelope of a spoken utterance. Fade-in and fade-out are applied to\n    avoid audible clicks.\n\n    In production, supply a real WAV recording of the customer query instead.\n    \"\"\"\n    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n\n    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n\n    # Mix frequencies that span the typical human speech range (100 Hz – 3 kHz)\n    components = [\n        (120,  1.0),\n        (250,  0.8),\n        (500,  0.6),\n        (800,  0.5),\n        (1600, 0.3),\n        (3000, 0.15),\n    ]\n    audio = sum(w * np.sin(2 * np.pi * f * t) for f, w in components)\n\n    # Fade in / out to eliminate clicks\n    fade = int(0.05 * sample_rate)\n    audio[:fade]  *= np.linspace(0, 1, fade)\n    audio[-fade:] *= np.linspace(1, 0, fade)\n\n    # Normalise to 70 % of full scale and convert to 16-bit PCM\n    audio = (audio / audio.max() * 0.70 * 32767).astype(np.int16)\n\n    wavfile.write(output_path, sample_rate, audio)\n    print(\n        f\"Sample query WAV created: {output_path} \"\n        f\"({duration:.1f}s, {sample_rate} Hz, 16-bit PCM)\"\n    )\n    return output_path\n\n\n# --- Generate synthetic input and run the full pipeline ---\nwav_path = _create_sample_query()\nresult   = handle_customer_query(wav_path)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-9-md",
   "metadata": {},
   "source": "### **7. Results**\n\nInspect the transcription and response text, then listen to or download the\nsynthesized audio reply."
  },
  {
   "cell_type": "code",
   "id": "cell-9",
   "metadata": {},
   "source": "from IPython.display import Audio, FileLink, display\n\nif result:\n    lang_label = _LANGUAGE_LABELS.get(result['language_code'], result['language_code'])\n\n    print(\"=== Pipeline Result ===\")\n    print(f\"Detected language : {lang_label} ({result['language_code']})\")\n    print(f\"Transcript        : {result['transcript'] or '(empty — synthetic audio)'}\")\n    print(f\"Response          : {result['response_text']}\")\n    print()\n    print(\"Audio reply:\")\n    display(Audio(filename=result['audio_path']))\n    print()\n    print(\"Download:\")\n    display(FileLink(result['audio_path'], result_html_prefix=\"Click to download: \"))\nelse:\n    print(\"Processing failed. Check the error messages above.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-10-md",
   "metadata": {},
   "source": "### **8. Error Reference**\n\n| Error | HTTP Status | Cause | Solution |\n| :--- | :--- | :--- | :--- |\n| `invalid_api_key_error` | 403 | Invalid API key | Verify at [dashboard.sarvam.ai](https://dashboard.sarvam.ai). |\n| `insufficient_quota_error` | 429 | Quota exceeded | Check your usage limits. |\n| `internal_server_error` | 500 | Server-side issue | Wait and retry the request. |\n| `WARNING: STT returned empty transcript` | — | Silent or synthetic audio | Use a real customer recording. |\n| `RuntimeError: no audio returned` | — | Unsupported language/speaker | Check `_SPEAKER_MAP` language code. |\n| `RuntimeError: SARVAM_API_KEY is not set` | — | Missing API key | Add key to `.env` file. |\n\n### **9. Using Real Customer Audio**\n\nReplace the synthetic demo with any real WAV or MP3 recording:\n\n```python\nresult = handle_customer_query(\"path/to/real_query.wav\")\n```\n\nThe pipeline auto-detects the spoken language and produces a reply in the same language.\n\n### **10. Conclusion & Resources**\n\nThis recipe chains **Saarika STT**, **Sarvam-M**, and **Bulbul TTS** into a production-ready\nvoice support loop that handles all major Indian languages out of the box.\n\n* [Sarvam AI Docs](https://docs.sarvam.ai)\n* [Saarika STT API](https://docs.sarvam.ai/api-reference-docs/speech-to-text)\n* [Sarvam-M Chat API](https://docs.sarvam.ai/api-reference-docs/chat)\n* [Bulbul TTS API](https://docs.sarvam.ai/api-reference-docs/text-to-speech)\n* [Indic Language Support](https://docs.sarvam.ai/language-support)\n\n**Keep Building!**"
  }
 ]
}